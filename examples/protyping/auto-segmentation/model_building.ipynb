{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOMATED BUILD OF MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings up u-net model architecture\n",
    "## down block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_block(down_input, m_2D_convs, n_3D_convs, c_output_channels, pool=True, crop=None):\n",
    "    \n",
    "    # RELU ACTIVATION LAYERS\n",
    "    # x: Input tensor.\n",
    "    # alpha: float. Slope of the negative part. Defaults to zero.\n",
    "    # max_value: float. Saturation threshold.\n",
    "    # threshold: float. Threshold value for thresholded activation.\n",
    "    \n",
    "    #down = tf.keras.layers.relu(down_input, alpha=0.0, max_value=None, threshold=0.0)\n",
    "    down = tf.keras.layers.Activation(\"relu\")(down_input)\n",
    "    \n",
    "    # CONVOLUTIONAL LAYERS\n",
    "    # filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "    \n",
    "    # kernel_size:\n",
    "    # 2D: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.\n",
    "    # Can be a single integer to specify the same value for all spatial dimensions\n",
    "    # 3D: An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. \n",
    "    # Can be a single integer to specify the same value for all spatial dimensions.\n",
    "    \n",
    "    # strides:\n",
    "    # 2D: An integer or tuple/list of 2 integers, specifying the strides of the convolution \n",
    "    # along the height and width. Can be a single integer to specify the same value for all spatial dimensions.\n",
    "    # 3D:  An integer or tuple/list of 3 integers, specifying the strides of the convolution along each spatial \n",
    "    # dimension. Can be a single integer to specify the same value for all spatial dimensions.\n",
    "    \n",
    "    # padding: One of \"valid\" or \"same\" (case-insensitive). \"valid\" means \"no padding\". \n",
    "    # \"same\" results in padding the input such that the output has the same length as the original input.\n",
    "    \n",
    "    # activation: Activation function to use (see activations). If you don't specify anything, \n",
    "    # no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "    \n",
    "    # kernel_initializer: Initializer for the kernel weights matrix\n",
    "\n",
    "    for i in range(m_2D_convs):\n",
    "        # repeat m times\n",
    "        \n",
    "        # 2D convolutional layer\n",
    "        # 1*3*3 same convolution\n",
    "        # Rel0 activation\n",
    "        down = tf.keras.layers.Conv3D(filters=c_output_channels,\n",
    "                                            kernel_size=(1,3,3),\n",
    "                                            strides=1,\n",
    "                                            activation='relu',\n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            padding='same')(down)\n",
    "    \n",
    "    for i in range(n_3D_convs):\n",
    "        # repeat loop n-1 times\n",
    "        \n",
    "        # 3D convolutional layer\n",
    "        # 1*3*3 same convolution\n",
    "        # Rel0 activation\n",
    "        down = tf.keras.layers.Conv3D(filters=c_output_channels,\n",
    "                                            # Note: Stated (1,3,3) convs\n",
    "                                            kernel_size=(1, 3, 3),\n",
    "                                            strides=1,\n",
    "                                            activation='relu',\n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            padding='same')(down)\n",
    "        \n",
    "        if i == n_3D_convs - 1: \n",
    "            # 3D convolutional layer\n",
    "            # 1*3*3 valid convolution\n",
    "            # No activation\n",
    "            down = tf.keras.layers.Conv3D(filters=c_output_channels,\n",
    "                                            # Note: Stated (3,1,1) convs\n",
    "                                            kernel_size=(3, 1, 1),\n",
    "                                            strides=1,\n",
    "                                            activation=None,\n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            padding='valid')(down)\n",
    "        \n",
    "        else:         \n",
    "            # Final 3D convolutional layer\n",
    "            # 1*3*3 valid convolution\n",
    "            # Rel0 activation\n",
    "            down = tf.keras.layers.Conv3D(filters=c_output_channels,\n",
    "                                            # Note: Stated (3,1,1) convs\n",
    "                                            kernel_size=(3, 1, 1),\n",
    "                                            strides=1,\n",
    "                                            activation='relu',\n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            padding='valid')(down)\n",
    "\n",
    "\n",
    "    \n",
    "    # Crop the image centrally in depth dimension\n",
    "    # could try tf.keras.layers.Cropping3D if this fails\n",
    "    if crop == None:\n",
    "        crop_central_depth = down_input[:]\n",
    "    else:\n",
    "        crop_central_depth = down_input[:, crop[0]:crop[1]]\n",
    "    \n",
    "    # Adapt channels - 1*1*1 conv to c channels\n",
    "    adapt_channels = tf.keras.layers.Conv3D(c_output_channels, (1, 1, 1), activation=None)(crop_central_depth)\n",
    "    \n",
    "    down = tf.keras.layers.Add()([down, adapt_channels])\n",
    "    \n",
    "            \n",
    "    # POOLING LAYERS 3D\n",
    "    # pool_size: tuple of 3 integers, factors by which to downscale (dim1, dim2, dim3). (2, 2, 2) will halve the size of the 3D input in each dimension.\n",
    "    # strides: tuple of 3 integers, or None. Strides values.\n",
    "    # padding: One of \"valid\" or \"same\" (case-insensitive).\n",
    "    # data_format: A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. \n",
    "    # channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while \n",
    "    # channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3). \n",
    "    # defaults to channels_last\n",
    "    \n",
    "    # 3D Average (not max) pooling layer\n",
    "    # 1*2*2\n",
    "    if pool == True:\n",
    "        pool = tf.keras.layers.AveragePooling3D(pool_size=(1, 2, 2), strides=None, padding='valid', data_format='channels_last')(down)\n",
    "        return down, pool\n",
    "    else:\n",
    "        return down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fc residual block\n",
    "### TODO: See NOTE below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_block(down_input, c_output_channels):\n",
    "    fc = tf.keras.layers.Conv3D(filters=c_output_channels,\n",
    "                                            kernel_size=(1,8,8),\n",
    "                                            strides=1,\n",
    "                                            activation=None,\n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            padding='valid')(down_input)\n",
    "    \n",
    "    for i in range(2):\n",
    "        crop = fc[:]\n",
    "        fc = tf.keras.layers.Activation(\"relu\")(fc)\n",
    "        \n",
    "        # NOTE: I believe there should be a fully connected layer here\n",
    "        fc = tf.keras.layers.Dense(1024)(fc)\n",
    "        fc = tf.keras.layers.Add()([fc, crop])\n",
    "    \n",
    "    fc = tf.keras.layers.Activation(\"relu\")(fc) \n",
    "    fc = tf.keras.layers.Reshape((1,8,8,256))(fc)\n",
    "    \n",
    "    return fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up block\n",
    "### TODO: Tensor shapes returned dont upscale correctly - see model assembly output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_block(down_input, up_input, m_2D_convs, c_output_channels, upscale=True):\n",
    "    \n",
    "    print(\"up_block\")\n",
    "    \n",
    "    print(\">>>> upblock arg down_in:\", down_input)\n",
    "    print(\">>>> upblock arg up_input:\", up_input)\n",
    "    \n",
    "    if upscale == True:\n",
    "        up = tf.keras.layers.UpSampling3D(size=(1,2,2))(up_input)\n",
    "        print(\">>>> upscale layer:\", up)\n",
    "    else:\n",
    "        up = up_input\n",
    "    \n",
    "    up = tf.keras.layers.concatenate([down_input, up], axis=0)\n",
    "    print(\">>>> concat layer:\", up)\n",
    "    \n",
    "    crop = up[:]\n",
    "    adapt_channels = tf.keras.layers.Conv3D(c_output_channels, (1, 1, 1), activation=None)(crop)\n",
    "    \n",
    "    up = tf.keras.layers.Activation(\"relu\")(up)\n",
    "    \n",
    "    for i in range(m_2D_convs):\n",
    "        up = tf.keras.layers.Conv3D(filters=c_output_channels,\n",
    "                                            # Note: Stated (1,3,3) convs\n",
    "                                            kernel_size=(1, 3, 3),\n",
    "                                            strides=1,\n",
    "                                            activation='relu',\n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            padding='same')(up)\n",
    "        print(\">>>> conv layer:\", up)\n",
    "        \n",
    "    up = tf.keras.layers.Add()([up, adapt_channels])\n",
    "    print(\">>>> add layer:\", up)\n",
    "    return up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(input_shape):\n",
    "    \n",
    "    # INPUT PATH\n",
    "    # Input level 1\n",
    "    input_1 = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # ENCODER PATH\n",
    "    # NOTE: Did not specify strides or padding for pooling\n",
    "    # Encoder level 1\n",
    "    down_1, pool_1 = down_block(input_1, 3, 0, 32)\n",
    "    print(\"down1:\", down_1)\n",
    "    print(\"pool1:\", pool_1)\n",
    "    print(\"---------------------\")\n",
    "    # Encoder level 2\n",
    "    down_2, pool_2 = down_block(pool_1, 3, 0, 32)\n",
    "    print(\"down2:\", down_2)\n",
    "    print(\"pool2:\", pool_2)\n",
    "    print(\"---------------------\")\n",
    "    # Encoder level 3\n",
    "    down_3, pool_3 = down_block(pool_2, 3, 0, 64)\n",
    "    print(\"down3:\", down_3)\n",
    "    print(\"pool3:\", pool_3)\n",
    "    print(\"---------------------\")\n",
    "    # Encoder level 4\n",
    "    down_4, pool_4 = down_block(pool_3, 1, 2, 64, crop=(2,19))\n",
    "    print(\"down4:\", down_4)\n",
    "    print(\"pool4:\", pool_4)\n",
    "    print(\"---------------------\")\n",
    "    # Encoder level 5\n",
    "    down_5, pool_5 = down_block(pool_4, 1, 2, 128, crop=(4,17))\n",
    "    print(\"down5:\", down_5)\n",
    "    print(\"pool5:\", pool_5)\n",
    "    print(\"---------------------\")\n",
    "    # Encoder level 6\n",
    "    down_6, pool_6 = down_block(pool_5, 1, 2, 128, crop=(4,17))\n",
    "    print(\"down6:\", down_6)\n",
    "    print(\"pool6:\", pool_6)\n",
    "    print(\"---------------------\")\n",
    "    # Encoder level 7 - No pooling\n",
    "    down_7 = down_block(pool_6, 0, 4, 256, pool=False, crop=(8,13))\n",
    "    print(\"down7:\", down_7)\n",
    "    print(\"---------------------\")\n",
    "\n",
    "    # FULLY CONNECTED PATH - level 8\n",
    "    fc_8 = fc_block(down_7, 1024)\n",
    "    print(\"fc8:\", fc_8)\n",
    "    print(\"---------------------\")\n",
    "\n",
    "    # DECODER PATH\n",
    "    # Decoder level 7 - No upscaling\n",
    "    up_7 = up_block(down_7, fc_8, 4, 256, upscale=False)\n",
    "    print(\"up7:\", up_7)\n",
    "    print(\"---------------------\")\n",
    "    # Decoder level 6\n",
    "    up_6 = up_block(down_6, up_7, 4, 128)\n",
    "    print(\"up6:\", up_6)\n",
    "    print(\"---------------------\")\n",
    "    # Decoder level 5\n",
    "    up_5 = up_block(down_5, up_6, 4, 128)\n",
    "    print(\"up5:\", up_5)\n",
    "    print(\"---------------------\")\n",
    "    # Decoder level 4\n",
    "    up_4 = up_block(down_4, up_5, 3, 64)\n",
    "    print(\"up4:\", up_4)\n",
    "    print(\"---------------------\")\n",
    "    # Decoder level 3\n",
    "    up_3 = up_block(down_3, up_4, 3, 64)\n",
    "    print(\"up3:\", up_3)\n",
    "    print(\"---------------------\")\n",
    "    # Decoder level 2\n",
    "    up_2 = up_block(down_2, up_3, 3, 64)\n",
    "    print(\"up2:\", up_2)\n",
    "    print(\"---------------------\")\n",
    "    # Decoder level 1\n",
    "    up_1 = up_block(down_1, up_2, 3, 64)\n",
    "    print(\"up1:\", up_1)\n",
    "    print(\"---------------------\")\n",
    "\n",
    "    # OUTPUT PATH\n",
    "    # Output level 1\n",
    "    # 1x1x1 sigmoid activation\n",
    "    output_1 = tf.keras.layers.Conv3D(1, (1, 1, 1), activation='sigmoid')(up_1)\n",
    "    print(\"output\", output_1)\n",
    "    print(\"---------------------\")\n",
    "    \n",
    "    return tf.keras.Model(inputs=input_1, outputs=output_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (21, 512, 512,1)\n",
    "model = unet_model(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = 'adam'\n",
    "LOSS = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "METRICS = ['accuracy']\n",
    "\n",
    "model.compile(optimizer = OPTIMIZER, loss = LOSS, metrics = METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model\n",
    "### TODO: dicom_process.load_data() - see dicom_process_example TODOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = dicom_process.load_data()\n",
    "print_data_split(train_images, test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Fit data to model\n",
    "model.fit(input_train, target_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=verbosity,\n",
    "          validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note if model performs better on training than test this is likely overfitting\n",
    "# Overfitting = memorising training data - does not generalise to other unseen cases\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import skimage.transform\n",
    "\n",
    "import skimage.draw\n",
    "\n",
    "import pydicom\n",
    "\n",
    "import numpy.ma\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# fix this\n",
    "from skimage.draw import polygon\n",
    "\n",
    "# dont include this in script\n",
    "plt.rcParams['figure.figsize'] = [6, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GETTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(data_path, ext):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of all files in data_path with ext dcm.\n",
    "    \"\"\"\n",
    "    if ext is None:\n",
    "        file_names = glob(data_path + \"/*\")\n",
    "    else:\n",
    "        file_names = glob(data_path + \"/*\" + ext)\n",
    "    file_names.sort()\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dicom_files(file_names):\n",
    "    \"\"\"\n",
    "    Returns a volume stack of DICOM files from names in list file_names.\n",
    "    \"\"\"\n",
    "    return [pydicom.dcmread(name, force=True) for name in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dicom_files(dicom_files):\n",
    "    \"\"\"\n",
    "    Filters a DICOM volume into 4 sections:\n",
    "\n",
    "    dicom_series: DICOM CT series files (CT DICOM files).\n",
    "\n",
    "    dicom_structures: DICOM structure set file (RS DICOM file).\n",
    "\n",
    "    dicom_plan: DICOM treatment plan file (RP DICOM file).\n",
    "\n",
    "    dicom_dose: DICOM dose grid file (RD DICOM file).\n",
    "    \"\"\"\n",
    "    dicom_series = []\n",
    "    dicom_structures = []\n",
    "    dicom_plan = []\n",
    "    dicom_dose = []\n",
    "\n",
    "    for file in dicom_files:\n",
    "        if hasattr(file, 'ImageType'):\n",
    "            dicom_series.append(file)\n",
    "        elif hasattr(file, 'StructureSetName'):\n",
    "            dicom_structures.append(file)\n",
    "        elif hasattr(file, 'BeamSequence'):\n",
    "            dicom_plan.append(file)\n",
    "        else:\n",
    "            dicom_dose.append(file)\n",
    "    return dicom_series, dicom_structures, dicom_plan, dicom_dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transfer_syntax(dicom_series):\n",
    "    \"\"\"\n",
    "    Fill in missing TransferSyntaxUID on DICOM files after reading\n",
    "    in the volume. Required before pixel_array attribute is called.\n",
    "    \"\"\"\n",
    "    for file in dicom_series:\n",
    "        try:\n",
    "            file.file_meta.TransferSyntaxUID\n",
    "        except AttributeError:\n",
    "            file.file_meta.TransferSyntaxUID = (\n",
    "                pydicom.uid.ImplicitVRLittleEndian)\n",
    "    return dicom_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_array(dicom_series):\n",
    "    \"\"\"\n",
    "    Return pixel array volume from DICOM imaging volume.\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        [file.pixel_array for file in dicom_series])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_structures(dicom_structures):\n",
    "    contours = []\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for i in range(len(dicom_structures.ROIContourSequence)):\n",
    "        contour = {}\n",
    "        contour['color'] = dicom_structures.ROIContourSequence[i].ROIDisplayColor\n",
    "        contour['number'] = dicom_structures.ROIContourSequence[i].ReferencedROINumber\n",
    "        contour['name'] = dicom_structures.StructureSetROISequence[i].ROIName\n",
    "        assert contour['number'] == dicom_structures.StructureSetROISequence[i].ROINumber\n",
    "        contour['contours'] = [s.ContourData for s in dicom_structures.ROIContourSequence[i].ContourSequence]\n",
    "        contours.append(contour)\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANIPULATING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_pixel_array(pixel_array, shape):\n",
    "    \"\"\"\n",
    "    Resizes axial slices in the pixel_array volume to (x, y)tuple scale.\n",
    "    Assumes volume shape (z, x, y) where z indexes axial slices.\n",
    "    \"\"\"\n",
    "    if len(pixel_array.shape) > 2:\n",
    "        shape = len(pixel_array), *shape\n",
    "    return skimage.transform.resize(pixel_array, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def resize_pixel_array(pixel_array, shape, is_mask=False):\n",
    "#    \"\"\"\n",
    "#    Resizes axial slices in the pixel_array volume to (x, y)tuple scale.\n",
    "##    Assumes volume shape (z, x, y) where z indexes axial slices.\n",
    "#    \"\"\"\n",
    "#    if len(pixel_array.shape) > 2:\n",
    "#        shape = len(pixel_array), *shape\n",
    "#    if is_mask:\n",
    "#        return numpy.ma.resize(pixel_array, shape)\n",
    "#    else:\n",
    "#        return skimage.transform.resize(pixel_array, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_pixel_array_volume(pixel_array_volume):\n",
    "    \"\"\"\n",
    "    Return a normalised pixel array volume\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    # Have changed from max of slice to max of array\n",
    "    # But have not put much thought into effect different\n",
    "    # maximum values from different patient cases may have (yet)!\n",
    "    # This should probably be /max for type\n",
    "    return pixel_array_volume / np.max(pixel_array_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_array(x, y, dicom_series):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    translation = dicom_series[0].ImagePositionPatient\n",
    "    scale = dicom_series[0].PixelSpacing\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    r = (y - translation[1]) / scale[1]\n",
    "    c = (x - translation[0]) / scale[0]\n",
    "    return -r, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simplified_names(names):\n",
    "    if len(names) > 1:\n",
    "        names = [name.lower().replace(' ', '').replace('_', '') for name in names]\n",
    "    else:\n",
    "        names[0].lower().replace(' ', '').replace('_', '')\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_masks(contours, slices, image, names):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    z = [np.around(s.ImagePositionPatient[2], 1) for s in slices]\n",
    "    label = np.zeros_like(image, dtype=np.int16)\n",
    "    for con in contours:\n",
    "        \n",
    "        # TODO\n",
    "        # Label selecting logic ie. using names and simplify\n",
    "        if con['name'] == \"Vacbag\":\n",
    "            \n",
    "            \n",
    "            num = int(con['number'])\n",
    "            for c in con['contours']:\n",
    "                nodes = np.array(c).reshape((-1, 3))\n",
    "                assert np.amax(np.abs(np.diff(nodes[:, 2]))) == 0\n",
    "                z_index = z.index(nodes[0, 2])\n",
    "                x = nodes[:, 0]\n",
    "                y = nodes[:, 1]\n",
    "                r, c = transform_to_array(x, y, slices)      \n",
    "                rr, cc = polygon(r, c)\n",
    "                try:\n",
    "                    label[z_index, rr, cc] = True\n",
    "                except IndexError:\n",
    "                    print(f\"IndexError for contour {con['name']} at {z_index}\")\n",
    "            \n",
    "        colors = tuple(np.array([con['color'] for con in contours]) / 255.0)\n",
    "    return label, colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTRUCT TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding(pixel_array_volume, index, padding=2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return pixel_array_volume[index - padding:index + padding + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(data, names=None):\n",
    "    file_names = list_files(patient, \".dcm\")\n",
    "    dicom_files = read_dicom_files(file_names)\n",
    "    dicom_series, dicom_structures, *rest = filter_dicom_files(dicom_files)\n",
    "    dicom_series = add_transfer_syntax(dicom_series)\n",
    "    dicom_series.sort(key=lambda x: float(x.ImagePositionPatient[2]))\n",
    "    images = get_pixel_array(dicom_series)\n",
    "    structures = read_structures(dicom_structures[0])\n",
    "    masks, colors = get_binary_masks(structures, dicom_series, images, [\"Vacbag\"])\n",
    "    return dicom_series, images, structures, masks, colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_model_data(shape, images, masks=None):\n",
    "    images = resize_pixel_array(images, shape)\n",
    "    images = normalise_pixel_array_volume(images)\n",
    "    if masks is not None:\n",
    "       # masks = resize_pixel_array(masks, shape, is_mask=True)\n",
    "        masks = resize_pixel_array(masks, shape)\n",
    "        masks = normalise_pixel_array_volume(masks)\n",
    "        masks = np.round(masks)\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_data(images, masks):\n",
    "    return model_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pixel_array(pixel_array, index=None):\n",
    "    \"\"\"\n",
    "    Quick hack to view a slice from either a 3D or 2D array\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    # Scale pixel intensity for CT\n",
    "    if index is not None:\n",
    "        pixel_array = pixel_array[index]\n",
    "    plt.imshow(pixel_array, cmap=plt.cm.bone)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_data(images, masks, index=90):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "    # plt.imshow(images[..., i + 90], cmap=\"gray\") # side\n",
    "    #plt.imshow(images[i + 90], cmap=\"gray\") # side\n",
    "        plt.imshow(images[i + index], cmap=\"gray\") # side\n",
    "    #plt.contour(masks[ i + 90], levels=[0.5, 1.5, 2.5, 3.5, 4.5], colors=colors)\n",
    "    #plt.contour(masks[ i + 90])#, colors=colors)\n",
    "        plt.contour(masks[ i + index])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_contour_data(structures):\n",
    "    for con in structures:\n",
    "        num = int(con['number'])\n",
    "        name = con['name']\n",
    "        print(f\"structures[{num}] = {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA FOR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/matthew/proj/masters-project/slim_DATASET\"\n",
    "data = list_files(data_root, None)\n",
    "shape = 64, 64\n",
    "\n",
    "loaded_data = []\n",
    "\n",
    "#good_data = [1, 2, 4, 6, 7, 8 , 9, 10, 16, 17, 20, 21]\n",
    "\n",
    "for index, patient in enumerate(data[0:1]):\n",
    "   # print(index+1, patient)\n",
    "    print(f\"LOADING: {index+1}/{len(data)}\")\n",
    "    print(f\"FILE: {patient}\")\n",
    "  #  if any( index + 1 == data for data in good_data):\n",
    "    try:\n",
    "        dicom_series, images, structures, masks, colors = get_training_data(patient)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    images, masks = shape_model_data(shape, images, masks)\n",
    "    loaded_data.append([images, masks])\n",
    "    print(f\"------- FILE COMPLETE ---------\")\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = np.array(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/home/matthew/proj/masters-project/test_data_array_2\", loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = 0\n",
    "index = 90\n",
    "images = loaded_data[patient][0]\n",
    "masks = loaded_data[patient][1]\n",
    "plot_model_data(images, masks, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 64\n",
    "#BUFFER_SIZE = 1000\n",
    "#STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_down( inputs , filters , stride_size ):\n",
    "    out = tf.nn.conv2d( inputs , filters , strides=stride_size , padding=padding ) \n",
    "    return tf.nn.leaky_relu( out , alpha=0.2 ) \n",
    "\n",
    "def maxpool_down( inputs , pool_size , stride_size ):\n",
    "    return tf.nn.max_pool( inputs , ksize=pool_size , padding='VALID' , strides=stride_size )\n",
    "\n",
    "def conv2d_up( inputs , filters , stride_size , output_shape ):\n",
    "    out = tf.nn.conv2d_transpose( inputs , filters , output_shape=output_shape , strides=stride_size , padding=padding ) \n",
    "    return tf.nn.leaky_relu( out , alpha=0.2 ) \n",
    "\n",
    "def maxpool_up( inputs , size ):\n",
    "    in_dimen = tf.shape( inputs )[ 1 ]\n",
    "    out_dimen = tf.cast( tf.round( in_dimen * size ) , dtype=tf.int32 ) \n",
    "    return tf.image.resize( inputs , [ out_dimen , out_dimen ] , method='nearest' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.initializers.glorot_uniform()\n",
    "def get_weight( shape , name ):\n",
    "    return tf.Variable( initializer( shape ) , name=name , trainable=True )\n",
    "\n",
    "shapes = [\n",
    "    [ 3 , 3 , 3 , 16 ] , \n",
    "    [ 3 , 3 , 16 , 16 ] , \n",
    "\n",
    "    [ 3 , 3 , 16 , 32 ] , \n",
    "    [ 3 , 3 , 32 , 32 ] ,\n",
    "\n",
    "    [ 3 , 3 , 32 , 64 ] , \n",
    "    [ 3 , 3 , 64 , 64 ] ,\n",
    "\n",
    "    [ 3 , 3 , 64 , 128 ] , \n",
    "    [ 3 , 3 , 128 , 128 ] ,\n",
    "\n",
    "    [ 3 , 3 , 128 , 256 ] , \n",
    "    [ 3 , 3 , 256 , 256 ] ,\n",
    "\n",
    "    [ 3 , 3 , 128 , 384 ],\n",
    "    [ 3 , 3 , 128 , 128 ],\n",
    "\n",
    "    [ 3 , 3 , 64 , 192 ],\n",
    "    [ 3 , 3 , 64 , 64 ],\n",
    "\n",
    "    [ 3 , 3 , 32 , 96 ],\n",
    "    [ 3 , 3 , 32 , 32 ],\n",
    "\n",
    "    [ 3 , 3 , 16 , 48 ],\n",
    "    [ 3 , 3 , 16 , 16 ],\n",
    "\n",
    "    [ 1 , 1 , 16 , 1 ],\n",
    "]\n",
    "\n",
    "weights = []\n",
    "for i in range( len( shapes ) ):\n",
    "    weights.append( get_weight( shapes[ i ] , 'weight{}'.format( i ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model( x ) :\n",
    "    batch_size = tf.shape( x )[0]\n",
    "    x = tf.cast( x , dtype=tf.float32 )\n",
    "    c1 = conv2d_down( x , weights[ 0 ] , stride_size=1 ) \n",
    "    c1 = conv2d_down( c1 , weights[ 1 ] , stride_size=1 ) \n",
    "    p1 = maxpool_down( c1 , pool_size=2 , stride_size=2 )\n",
    "    \n",
    "    c2 = conv2d_down( p1 , weights[ 2 ] , stride_size=1 )\n",
    "    c2 = conv2d_down( c2 , weights[ 3 ] , stride_size=1 ) \n",
    "    p2 = maxpool_down( c2 , pool_size=2 , stride_size=2 )\n",
    "    \n",
    "    c3 = conv2d_down( p2 , weights[ 4 ] , stride_size=1 ) \n",
    "    c3 = conv2d_down( c3 , weights[ 5 ] , stride_size=1 ) \n",
    "    p3 = maxpool_down( c3 , pool_size=2 , stride_size=2 )\n",
    "    \n",
    "    c4 = conv2d_down( p3 , weights[ 6 ] , stride_size=1 )\n",
    "    c4 = conv2d_down( c4 , weights[ 7 ] , stride_size=1 )\n",
    "    p4 = maxpool_down( c4 , pool_size=2 , stride_size=2 )\n",
    "\n",
    "    c5 = conv2d_down( p4 , weights[ 8 ] , stride_size=1 )\n",
    "    c5 = conv2d_down( c5 , weights[ 9 ] , stride_size=1 ) \n",
    "        \n",
    "    p5 = maxpool_up( c5 , 2 )\n",
    "    concat_1 = tf.concat( [ p5 , c4 ] , axis=-1 ) \n",
    "    c6 = conv2d_up( concat_1 , weights[ 10 ] , stride_size=1 , output_shape=[ batch_size , 16 , 16 , 128 ] )\n",
    "    c6 = conv2d_up( c6 , weights[ 11 ] , stride_size=1 , output_shape=[ batch_size , 16 , 16 , 128 ] )  \n",
    "\n",
    "    p6 = maxpool_up( c6 , 2 )\n",
    "    concat_2 = tf.concat( [ p6 , c3 ] , axis=-1 ) \n",
    "    c7 = conv2d_up( concat_2 , weights[ 12 ] , stride_size=1 , output_shape=[ batch_size , 32 , 32 , 64 ] )\n",
    "    c7 = conv2d_up( c7 , weights[ 13 ] , stride_size=1 , output_shape=[ batch_size , 32 , 32 , 64 ] )  \n",
    "\n",
    "    p7 = maxpool_up( c7 , 2 )\n",
    "    concat_3 = tf.concat( [ p7 , c2 ] , axis=-1 ) \n",
    "    c8 = conv2d_up( concat_3 , weights[ 14 ] , stride_size=1 , output_shape=[ batch_size , 64 , 64 , 32 ] )\n",
    "    c8 = conv2d_up( c8 , weights[ 15 ] , stride_size=1 , output_shape=[ batch_size , 64 , 64 , 32 ] )   \n",
    "\n",
    "    p8 = maxpool_up( c8 , 2 )\n",
    "    concat_4 = tf.concat( [ p8 , c1 ] , axis=-1 ) \n",
    "    c9 = conv2d_up( concat_4 , weights[ 16 ] , stride_size=1 , output_shape=[ batch_size , 128 , 128 , 16 ] )\n",
    "    c9 = conv2d_up( c9 , weights[ 17 ] , stride_size=1 , output_shape=[ batch_size , 128 , 128 , 16 ] )   \n",
    "\n",
    "    output = tf.nn.conv2d( c9 , weights[ 18 ] , strides=[ 1 , 1 , 1 , 1 ] , padding=padding ) \n",
    "    outputs = tf.nn.sigmoid( output ) \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss( pred , target ):\n",
    "    return tf.losses.binary_crossentropy( target , pred )\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.optimizers.Adam( learning_rate=learning_rate )\n",
    "\n",
    "def train( model, inputs , outputs ):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss( model( inputs ), outputs)\n",
    "    grads = tape.gradient( current_loss , weights )\n",
    "    optimizer.apply_gradients( zip( grads , weights ) )\n",
    "    print( tf.reduce_mean( current_loss ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25  #@param {type: \"number\"}\n",
    "\n",
    "for e in range( num_epochs ):\n",
    "    print( 'Epoch {} out of {} {}'.format( e + 1 , num_epochs , '--' * 50 ) )\n",
    "    for patient in loaded_data:\n",
    "        print(len(range(patient)))\n",
    "        #for index in len(range(patient)):\n",
    "         #   image, label = patient[index]\n",
    "         #   train( model , image , label )\n",
    "        \n",
    "#    patient = 0\n",
    "#    index = 90\n",
    "#    images = loaded_data[patient][0]\n",
    "#    masks = loaded_data[patient][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypymedphys",
   "language": "python",
   "name": "mypymedphys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
